### Technical Overview: /components/import

This directory contains the frontend implementation for the book ingestion pipeline, facilitating the migration of library data from external sources such as Goodreads, StoryGraph, and plain text files. The architecture follows a multi-phase state machine (Parsing → Previewing → Committing → Success) managed by a central orchestrator.

#### Key File Roles

- **`ImportFlow.tsx`**: The primary orchestrator of the import lifecycle. It integrates the `useImportJob` hook to manage transition states, handles analytics logging via `logImportEvent`, and coordinates post-import side effects like automatic book cover backfilling.
- **`UploadDropzone.tsx`**: A specialized file input component that manages drag-and-drop interactions. It performs client-side validation for file size (10MB limit) and file extensions (`.csv`, `.txt`, `.md`) before initiating the extraction process.
- **`PreviewTable.tsx`**: A data grid for reviewing parsed books before they are committed to the database. It supports batch actions (Create, Merge, Skip) and implements intelligent default actions based on deduplication confidence scores.
- **`DedupControls.tsx`**: A specialized sub-component for the preview grid that interfaces with the database via Convex queries. It provides real-time lookups of existing books to help users resolve potential duplicates by comparing titles, authors, and ISBNs.
- **`ExtractionProgress.tsx`**: A feedback component used during long-running asynchronous operations. It features a deterministic CSS-animated "bookshelf" loading state, rotating literary quotes, and elapsed time tracking to improve UX during server-side processing.
- **`CommitSummary.tsx`**: The terminal state component that reports the final tally of books created, merged, and skipped, providing navigation back to the library or a path to restart the flow.

#### Architecture and Data Flow

The directory utilizes a modular UI pattern where individual components represent specific phases of the import job. Data flows from the `UploadDropzone` to Convex actions for extraction, returns to the `PreviewTable` for user verification, and is finally persisted through a commit mutation.

The deduplication logic is a hybrid of automated confidence scoring and manual intervention. High-confidence matches (≥85%) default to a "merge" action, while lower-confidence matches default to "skip," requiring the user to explicitly choose a resolution in `DedupControls`.

#### Key Dependencies and Implementation Details

- **Convex API**: Deeply integrated via `useAction` and `useMutation` for server-side extraction and database persistence.
- **State Management**: Relies on a custom `useImportJob` hook (located in `@/hooks/useImportJob`) to encapsulate the complex transitions between parsing and committing.
- **Styling & UI**: Built with Tailwind CSS and a custom design system (`Surface`, `Button`, `canvas-bone`). It uses `framer-motion` concepts (via `motion-fade-in`) and standard CSS keyframes for animations.
- **Environment Flags**: The flow respects `NEXT_PUBLIC_IMPORT_ENABLED` and `NEXT_PUBLIC_COVER_BACKFILL_ENABLED` flags to toggle functionality at runtime.
- **Gotchas**:
  - The cover backfill process is triggered as a fire-and-forget side effect upon a "success" status, limited to 20 books per batch.
  - Component state for row selection in `PreviewTable` is managed locally and cleared after batch actions.
  - File validation is strictly extension-based on the client side; actual content parsing occurs on the server.
