The `lib/import` directory serves as the core orchestration layer for the application's book ingestion pipeline. It facilitates a multi-stage workflow: parsing (structured or unstructured), deduplication against existing user data, rate-limiting, and final commitment to the database. The architecture is designed to handle both deterministic client-side parsing (for CSVs and specific Markdown formats) and non-deterministic server-side extraction (via LLMs for unstructured text).

### Key File Roles

- **`llm.ts`**: The primary engine for unstructured data extraction. It implements a sophisticated chunking and parallel-processing strategy to send text to OpenRouter-hosted LLMs. It features a dual-prompt architecture: one for strict JSON extraction and a second "verifier" pass to ensure no books were omitted. It includes robust retry logic and exponential backoff for network resilience.
- **`dedup.ts`**: Acts as the high-level coordinator for duplicate detection. It bridges the core matching algorithms with the database, providing functions to `findMatches`, `applyDecision` (generating patches for existing records), and `buildNewBook` (mapping `ParsedBook` objects to the final database schema).
- **`types.ts`**: The central schema definition file. It defines the `ParsedBook` interface, which acts as the "Intermediate Representation" for all parsers. It also contains Convex-specific validators (`v.object`) to ensure data integrity during transit between the client and server.
- **`normalize.ts`**: Provides string-processing utilities essential for fuzzy matching. It performs ASCII folding (removing diacritics), punctuation stripping, and whitespace collapse to generate stable "Title|Author" keys for the deduplication engine.
- **`status.ts`**: Manages the mapping of external "shelves" or "statuses" (e.g., "to-read", "currently_reading") into the application's internal `ImportStatus` enum.
- **`rateLimit.ts`**: Implements governance logic to prevent system abuse. It enforces daily import caps and prevents concurrent import runs per user by querying recent activity from the `ImportRunRepository`.
- **`metrics.ts`**: Provides structured, PII-free logging for import events, tracking phase duration, token usage, and row counts for observability.

### Architecture and Data Flow

The directory orchestrates a pipeline where data is first transformed into a `ParsedBook` array.

1.  **Parsing**: Structured files (CSV/Goodreads) are handled by client-side logic in the `client/` subdirectory. Unstructured text is routed to the server-side `llm.ts` logic.
2.  **Deduplication**: Incoming rows are compared against the user's library using the `dedup/` sub-module, resulting in `DedupMatch` objects with varying confidence scores (ISBN=1.0, Title-Author=0.8, API-ID=0.6).
3.  **Preview/Decision**: The system generates a `PreviewResult`, allowing users to choose between `skip`, `merge`, or `create` for each book.
4.  **Commit**: Validated decisions are processed by the repository layer to update or insert records in the Convex database.

### Important Dependencies and Gotchas

- **Execution Environment**: `llm.ts` contains server-only logic and will throw an error if imported into a client-side bundle, as it relies on sensitive API keys and OpenRouter integrations.
- **Database Invariants**: The `buildNewBook` and `applyDecision` functions in `dedup.ts` enforce specific business rules, such as ensuring that any book with a "read" status has a `timesRead` count of at least 1, regardless of the source data.
- **LLM Token Limits**: While the system supports large-context models, it enforces a `LLM_TOKEN_CAP` (500,000 tokens) to prevent runaway costs and processing timeouts. Token estimation is performed using a character-to-token heuristic (divisor of 4).
- **Repository Abstraction**: The module is designed to be database-agnostic through the `repository/` interfaces, allowing the entire import logic to be unit-tested using the `memory.ts` implementation without a live Convex connection.
- **Normalization Sensitivity**: Deduplication relies heavily on `normalize.ts`. Changes to punctuation stripping or ASCII folding logic can result in different "Title|Author" keys, potentially leading to duplicate entries if the normalization logic is not kept consistent across the library.
