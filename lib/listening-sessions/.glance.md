This directory contains the core logic for the "listening sessions" feature, which transforms spoken user reflections into structured reading artifacts. The module manages the entire lifecycle of a session, from audio transcription and library context aggregation to LLM-based synthesis and cost governance.

### Architecture and Key Roles

The module follows a pipeline architecture that moves data through transcription, context preparation, and synthesis stages:

- **Transcription and Audio Management (`transcription.ts`, `mime.ts`)**: Handles the retrieval of audio from trusted hosts (e.g., Vercel Blob) and coordinates speech-to-text (STT) via ElevenLabs (primary) and Deepgram (fallback). It includes strict byte-limit enforcement and mime-type normalization.
- **Context Aggregation (`contextPacker.ts`)**: Implements a deterministic algorithm to select and prioritize user data (books and notes) for inclusion in the LLM prompt. It manages token budgeting through character-based estimation, applies a "diversity cap" to prevent single-book dominance, and handles privacy redactions.
- **Synthesis and Prompt Engineering (`synthesisPrompt.ts`, `synthesisSchema.ts`, `synthesisConfig.ts`)**: Constructs the system instructions and data payloads for the LLM. It utilizes a strict JSON schema to ensure the LLM returns consistent artifacts (insights, open questions, quotes, etc.) and manages model selection and fallback logic via OpenRouter.
- **Entitlements and Governance (`entitlements.ts`, `cost-estimation.ts`, `cost-guardrails.ts`)**: Validates user access levels and subscription status via Convex. It also calculates estimated USD costs per session based on token usage and enforces hard caps to prevent runaway API spend.
- **Resiliency (`fallback-artifacts.ts`, `pipeline-stages.ts`)**: Provides regex-based heuristic extraction to generate basic artifacts if LLM synthesis fails, ensuring the user receives a result even during API outages.

### Key Dependencies and Technical Constraints

- **Convex**: Heavily integrated for state persistence, rate limiting, and subscription verification.
- **External API Providers**: Relies on ElevenLabs/Deepgram for STT and OpenRouter for LLM synthesis.
- **Token Estimation Heuristic**: The logic uses a character-to-token ratio (approximately 4 characters per token) rather than a formal BPE tokenizer. This approximation is used for real-time budgeting in the `contextPacker`.
- **Server-Side Constraints**: The `entitlements.ts` logic is strictly server-only, as it interfaces with sensitive authentication templates and the Convex HTTP client.
- **Deterministic Ranking**: The `contextPacker` uses a scoring system combining recency and "current book" bonuses to ensure that context generation is stable and reproducible for a given library state.
- **Non-Negotiable Metadata**: Book titles and authors are treated as essential overhead; they are included in token counts even if the budget is highly restricted, which can lead to content stripping if the budget is set too low.
