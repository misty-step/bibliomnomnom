This directory contains the test suite for the "Listening Sessions" module of the Bibliomnomnom application. Its primary purpose is to validate the end-to-end pipeline responsible for capturing audio, transcribing speech, and using Large Language Models (LLMs) to synthesize reading-related insights, quotes, and follow-up questions.

### Architecture and Key File Roles

The test suite reflects a multi-stage processing architecture—defined in `pipeline-stages.test.ts` as recording, uploading, transcribing, synthesizing, and completing. The architecture relies on several distinct functional pillars:

- **LLM Synthesis and Prompting**: `synthesisPrompt.test.ts` and `synthesis.test.ts` validate the construction of complex prompts that inject user context (current books, recent notes) into the LLM. They also ensure that the resulting AI-generated artifacts are normalized, sanitized, and clamped to specific length constraints to maintain data integrity.
- **Cost Management and Guardrails**: `cost-estimation.test.ts` and `cost-guardrails.test.ts` handle the financial logic of the feature. These files test the logic for calculating costs across different providers (OpenAI, Anthropic, and Google) and verify that the system triggers warnings or hard stops when sessions exceed predefined USD thresholds.
- **Reliability and Fallbacks**: `fallback-artifacts.test.ts` verifies a heuristic-based extraction system that generates basic insights, quotes, and questions directly from text. This ensures the system provides value even if the LLM synthesis stage fails or is bypassed.
- **Infrastructure and Access Control**: `entitlements.test.ts` tests the integration between the application, Clerk (authentication), and Convex (backend database). It validates subscription checks, trial creation logic, and rate-limiting enforcement.
- **Configuration and Environment**: `synthesisConfig.test.ts` ensures that environment variables for LLM parameters (model selection, temperature, top_p) are parsed correctly, with appropriate defaults and safety clamping for specific model families like GPT-5.
- **Data Ingestion**: `mime.test.ts` handles the normalization of audio metadata, ensuring various MIME types are correctly mapped to supported file extensions for the transcription engine.

### Key Dependencies and Gotchas

- **Vitest**: The suite uses Vitest as the test runner, employing `vi.mock` and `vi.hoisted` for complex dependency injection, particularly for the `ConvexHttpClient`.
- **Multi-Provider Token Logic**: A significant "gotcha" addressed in `cost-estimation.test.ts` is the discrepancy between provider usage formats (e.g., OpenAI’s `prompt_tokens` vs. Anthropic’s `input_tokens`). The logic includes specific fallbacks to derive prompt counts from total tokens if fields are missing.
- **Model-Specific Defaults**: The configuration logic contains specific overrides for certain models (e.g., omitting temperature by default for OpenAI reasoning models) which must be accounted for during environment setup.
- **Context Truncation**: The prompt builder is designed to aggressively truncate reading lists and notes to fit within LLM context windows, a behavior verified in `synthesisPrompt.test.ts`.
- **Strict Cost Comparison**: Guardrails use strict greater-than logic for hard caps, meaning a session exactly at the hard cap is treated as a warning rather than a terminal error.
