The `/docs/performance` directory serves as the central documentation and benchmarking hub for the Speech-to-Text (STT) integration layer of the `bibliomnomnom` book annotation system. Its primary purpose is to define the selection criteria, cost-benefit analysis, and failure-handling logic for processing long-form audio sessions ranging from 30 minutes to 4 hours.

### Architecture and Workflow

The system utilizes a batch-processing architecture rather than a real-time streaming model. The workflow follows a linear path: audio recording, persistence to Vercel Blob storage, asynchronous transcription via external REST APIs, and final synthesis.

The architecture implements a tiered redundancy strategy:

1.  **Primary Provider:** ElevenLabs Scribe v2, selected for its high accuracy with academic vocabulary and lack of duration caps.
2.  **Fallback Provider:** Deepgram Nova-3, triggered by 5xx errors, timeouts (>120s), or rate limits.
3.  **Persistence Layer:** If both providers fail, the system preserves the audio blob and initiates an exponential backoff retry queue (1h, 4h, 24h intervals) managed via cron jobs.

### Key File Roles

- **`stt-decision-matrix.md`**: Acts as the authoritative governance document for STT integration. It contains a comparative scoring matrix across three providers (ElevenLabs, Deepgram, and AssemblyAI) evaluating accuracy (WER), cost, session duration limits, and operational complexity. It also defines the "Kill Switch" policy and cost projections for growth targets.
- **`stt-eval-raw-template.json`**: Provides a standardized JSON schema for benchmark results generated by the `scripts/stt-eval.py` utility. It tracks metadata such as audio duration and cost-per-minute alongside per-provider performance metrics like latency, character counts, and consensus transcripts derived from plurality voting.

### Dependencies and Gotchas

- **Dependencies:** The benchmarking and integration suite requires Pythonâ€™s `requests` library and environment variables for `ELEVENLABS_API_KEY`, `DEEPGRAM_API_KEY`, and `ASSEMBLYAI_API_KEY`.
- **Session Caps:** A significant constraint is the 3-hour hard cap on AssemblyAI Universal-Streaming (WebSocket code 3005), which led to its rejection in favor of ElevenLabs and Deepgram to support the application's 4-hour maximum session duration.
- **Architectural Mismatch:** AssemblyAI Universal-Streaming was found incompatible with the current server-side processing model, as it requires a persistent browser-side connection during capture.
- **Benchmarking Nuance:** While AssemblyAI's streaming product was rejected for production, the `stt-eval.py` script is configured to benchmark their **Universal-2 (batch)** product as a potential tertiary fallback.
